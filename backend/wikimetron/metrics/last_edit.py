#!/usr/bin/env python3
"""
Analyse enrichie des derni√®res modifications sur des pages Wikip√©dia.

Informations collect√©es:
  ‚Ä¢ Score de r√©cence (0=r√©cent, 1=ancien)
  ‚Ä¢ Dernier √©diteur et commentaire
  ‚Ä¢ Taille de la page et changement
  ‚Ä¢ Nombre d'√©dits r√©cents (7j, 30j, 90j)
  ‚Ä¢ M√©tadonn√©es de la page (cr√©ation, protection, redirections)
  ‚Ä¢ Analyse d'activit√©

Usage:
    python last_edit_enhanced.py "Page 1" "Page 2" --lang fr --max_days 365 --detailed
"""
import requests
import pandas as pd
import time
import json
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional
import argparse

UA = {"User-Agent": "WikiAnalyzer/2.0 (analysis@example.com)"}

class WikiPageAnalyzer:
    def __init__(self, lang: str = "fr"):
        self.lang = lang
        self.api_url = f"https://{lang}.wikipedia.org/w/api.php"
        
    def get_page_info(self, title: str) -> Dict:
        """R√©cup√®re les informations de base de la page."""
        params = {
            "action": "query",
            "format": "json",
            "titles": title,
            "prop": "info|pageprops",
            "inprop": "protection|created|talkid|url|size",
            "ppprop": "disambiguation|redirect"
        }
        
        try:
            r = requests.get(self.api_url, headers=UA, params=params, timeout=15)
            r.raise_for_status()
            data = r.json()
            
            page = next(iter(data["query"]["pages"].values()))
            
            if "missing" in page:
                return {"exists": False, "title": title}
                
            return {
                "exists": True,
                "title": page.get("title", title),
                "pageid": page.get("pageid"),
                "size": page.get("size", 0),
                "created": page.get("created"),
                "url": page.get("fullurl", ""),
                "is_redirect": "redirect" in page.get("pageprops", {}),
                "is_disambiguation": "disambiguation" in page.get("pageprops", {}),
                "protection": page.get("protection", [])
            }
        except Exception as e:
            return {"exists": False, "title": title, "error": str(e)}
    
    def get_recent_revisions(self, title: str, days: int = 90, limit: int = 500) -> List[Dict]:
        """R√©cup√®re les r√©visions r√©centes."""
        cutoff = datetime.now(timezone.utc) - timedelta(days=days)
        cutoff_str = cutoff.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        params = {
            "action": "query",
            "format": "json",
            "titles": title,
            "prop": "revisions",
            "rvprop": "timestamp|user|comment|size|ids",
            "rvlimit": limit,
            "rvend": cutoff_str,  # ‚úÖ S'arr√™te il y a X jours
            "rvdir": "older"      # ‚úÖ Du pr√©sent vers le pass√©
        }
        
        try:
            r = requests.get(self.api_url, headers=UA, params=params, timeout=15)
            r.raise_for_status()
            data = r.json()
            
            page = next(iter(data["query"]["pages"].values()))
            return page.get("revisions", [])
        except Exception:
            return []
    
    def get_latest_revision(self, title: str) -> Optional[Dict]:
        """R√©cup√®re la derni√®re r√©vision avec d√©tails."""
        params = {
            "action": "query",
            "format": "json",
            "titles": title,
            "prop": "revisions",
            "rvprop": "timestamp|user|comment|size|ids",
            "rvlimit": 1
        }
        
        try:
            r = requests.get(self.api_url, headers=UA, params=params, timeout=15)
            r.raise_for_status()
            data = r.json()
            
            page = next(iter(data["query"]["pages"].values()))
            revisions = page.get("revisions", [])
            return revisions[0] if revisions else None
        except Exception:
            return None
    
    def analyze_page(self, title: str, max_days: int = 365) -> Dict:
        """Analyse compl√®te d'une page."""
        print(f"üîç Analyse de: {title}")
        
        # Informations de base
        page_info = self.get_page_info(title)
        if not page_info["exists"]:
            return {
                "title": title,
                "exists": False,
                "error": page_info.get("error", "Page inexistante"),
                "recency_score": 1.0
            }
        
        # Derni√®re r√©vision
        latest_rev = self.get_latest_revision(title)
        if not latest_rev:
            return {
                "title": title,
                "exists": True,
                "error": "Impossible de r√©cup√©rer les r√©visions",
                "recency_score": 1.0
            }
        
        # Calcul du score de r√©cence
        last_edit_dt = datetime.fromisoformat(latest_rev["timestamp"].replace("Z", "+00:00"))
        now_utc = datetime.now(timezone.utc)
        days_since_edit = (now_utc - last_edit_dt).days
        recency_score = min(1.0, days_since_edit / max_days)
        
        # R√©visions r√©centes pour analyse d'activit√©
        recent_revs_90d = self.get_recent_revisions(title, days=90, limit=500)
        recent_revs_30d = [r for r in recent_revs_90d if self._days_ago(r["timestamp"]) <= 30]
        recent_revs_7d = [r for r in recent_revs_90d if self._days_ago(r["timestamp"]) <= 7]
        
        # Analyse des utilisateurs actifs
        users_90d = set(r.get("user", "Anonyme") for r in recent_revs_90d)
        users_30d = set(r.get("user", "Anonyme") for r in recent_revs_30d)
        
        # Calcul de la taille moyenne des changements
        size_changes = []
        for i in range(1, min(len(recent_revs_90d), 20)):  # 20 derniers changements
            if "size" in recent_revs_90d[i-1] and "size" in recent_revs_90d[i]:
                change = recent_revs_90d[i-1]["size"] - recent_revs_90d[i]["size"]
                size_changes.append(change)
        
        avg_change = sum(size_changes) / len(size_changes) if size_changes else 0
        
        # Classification de l'activit√©
        activity_level = self._classify_activity(len(recent_revs_30d), len(users_30d))
        
        return {
            "title": title,
            "exists": True,
            "pageid": page_info["pageid"],
            "url": page_info["url"],
            "current_size": page_info["size"],
            "created": page_info.get("created"),
            "is_redirect": page_info["is_redirect"],
            "is_disambiguation": page_info["is_disambiguation"],
            "protection": len(page_info["protection"]) > 0,
            
            # Derni√®re r√©vision
            "last_edit_timestamp": latest_rev["timestamp"],
            "last_edit_user": latest_rev.get("user", "Anonyme"),
            "last_edit_comment": latest_rev.get("comment", "")[:100] + ("..." if len(latest_rev.get("comment", "")) > 100 else ""),
            "days_since_last_edit": days_since_edit,
            "recency_score": round(recency_score, 4),
            
            # Activit√© r√©cente
            "edits_7d": len(recent_revs_7d),
            "edits_30d": len(recent_revs_30d),
            "edits_90d": len(recent_revs_90d),
            "unique_users_30d": len(users_30d),
            "unique_users_90d": len(users_90d),
            "avg_size_change": round(avg_change, 1),
            "activity_level": activity_level,
            
            # M√©ta
            "analysis_timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    def _days_ago(self, timestamp: str) -> int:
        """Calcule le nombre de jours depuis un timestamp."""
        dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
        return (datetime.now(timezone.utc) - dt).days
    
    def _classify_activity(self, edits_30d: int, users_30d: int) -> str:
        """Classifie le niveau d'activit√© de la page."""
        if edits_30d == 0:
            return "üî¥ INACTIVE"
        elif edits_30d <= 2:
            return "üü° FAIBLE"
        elif edits_30d <= 10:
            return "üü¢ MOD√âR√âE"
        elif edits_30d <= 30:
            return "üîµ √âLEV√âE"
        else:
            return "üü£ TR√àS √âLEV√âE"


def analyze_pages(pages: List[str], lang: str = "fr", max_days: int = 365, detailed: bool = False) -> pd.DataFrame:
    """Analyse un ensemble de pages et retourne un DataFrame."""
    analyzer = WikiPageAnalyzer(lang)
    results = []
    
    print(f"üöÄ Analyse de {len(pages)} page(s) sur {lang}.wikipedia.org")
    print("=" * 60)
    
    for i, page in enumerate(pages, 1):
        print(f"[{i}/{len(pages)}] ", end="")
        result = analyzer.analyze_page(page, max_days)
        results.append(result)
        time.sleep(0.2)  # Rate limiting
    
    df = pd.DataFrame(results)
    
    # R√©ordonne les colonnes pour une meilleure lisibilit√©
    if not df.empty and detailed:
        column_order = [
            "title", "exists", "recency_score", "days_since_last_edit",
            "activity_level", "edits_30d", "edits_90d", "unique_users_30d",
            "current_size", "avg_size_change", "last_edit_user", "last_edit_comment",
            "is_redirect", "is_disambiguation", "protection", "url"
        ]
        # Garde seulement les colonnes qui existent
        available_cols = [col for col in column_order if col in df.columns]
        df = df[available_cols + [col for col in df.columns if col not in available_cols]]
    
    return df


def main():
    parser = argparse.ArgumentParser(
        description="Analyse enrichie des derni√®res modifications sur des pages Wikip√©dia"
    )
    parser.add_argument("pages", nargs="+", help="Titres d'articles Wikip√©dia")
    parser.add_argument("--lang", default="fr", help="Code langue (fr, en, ‚Ä¶)")
    parser.add_argument("--max_days", type=int, default=365, 
                       help="Nombre de jours pour score maximal (d√©faut=365)")
    parser.add_argument("--detailed", action="store_true", 
                       help="Affichage d√©taill√© avec toutes les colonnes")
    parser.add_argument("--output", help="Fichier de sortie CSV (optionnel)")
    parser.add_argument("--json", action="store_true", 
                       help="Sortie au format JSON au lieu de tableau")
    
    args = parser.parse_args()
    
    # Analyse
    start_time = time.time()
    df = analyze_pages(args.pages, args.lang, args.max_days, args.detailed)
    end_time = time.time()
    
    print(f"\n‚è±Ô∏è  Analyse termin√©e en {end_time - start_time:.1f}s")
    print("=" * 60)
    
    if df.empty:
        print("‚ùå Aucune donn√©e r√©cup√©r√©e")
        return
    
    # Affichage des r√©sultats
    if args.json:
        result_json = df.to_json(orient="records", indent=2, force_ascii=False)
        print(result_json)
    else:
        if args.detailed:
            # Affichage d√©taill√©
            for _, row in df.iterrows():
                print(f"\nüìÑ {row['title']}")
                if not row.get('exists', True):
                    print(f"   ‚ùå {row.get('error', 'Page inexistante')}")
                    continue
                    
                print(f"   üéØ Score de r√©cence: {row['recency_score']:.3f} ({row['days_since_last_edit']} jours)")
                print(f"   üìà Activit√©: {row['activity_level']} ({row['edits_30d']} √©dits/30j)")
                print(f"   üë§ Dernier √©diteur: {row['last_edit_user']}")
                print(f"   üí¨ Commentaire: {row['last_edit_comment']}")
                print(f"   üìä Taille: {row['current_size']:,} octets (Œî moy: {row['avg_size_change']:+.1f})")
                
                flags = []
                if row.get('is_redirect'): flags.append("‚Ü™Ô∏è Redirection")
                if row.get('is_disambiguation'): flags.append("üîÄ Homonymie")
                if row.get('protection'): flags.append("üîí Prot√©g√©e")
                if flags:
                    print(f"   üè∑Ô∏è  {' | '.join(flags)}")
        else:
            # Affichage r√©sum√©
            summary_cols = ["title", "recency_score", "days_since_last_edit", "activity_level", "edits_30d"]
            available_summary = [col for col in summary_cols if col in df.columns]
            print(df[available_summary].to_string(index=False))
    
    # Sauvegarde si demand√©e
    if args.output:
        df.to_csv(args.output, index=False)
        print(f"\nüíæ R√©sultats sauvegard√©s dans: {args.output}")
    
    # Statistiques rapides
    if len(df) > 1 and not args.json:
        existing_pages = df[df.get('exists', True) == True]
        if not existing_pages.empty:
            print(f"\nüìà STATISTIQUES:")
            print(f"   Score moyen de r√©cence: {existing_pages['recency_score'].mean():.3f}")
            print(f"   Page la plus r√©cente: {existing_pages.loc[existing_pages['recency_score'].idxmin(), 'title']}")
            print(f"   Page la plus ancienne: {existing_pages.loc[existing_pages['recency_score'].idxmax(), 'title']}")

def get_recency_score(pages, lang="fr", max_days=365):
    """
    Calcule le score de r√©cence pour chaque page Wikipedia.
    Retourne une Series index√©e par page, valeurs entre 0 (r√©cent) et 1 (ancien).
    """
    df = analyze_pages(pages, lang, max_days)
    # Si tu veux que le nom de la m√©trique soit explicite
    series = pd.Series(df.set_index("title")["recency_score"])
    series.name = "recency_score"
    return series

if __name__ == "__main__":
    main()